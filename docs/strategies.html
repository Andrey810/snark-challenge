<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8">
    <link rel="stylesheet" type="text/css" href="/snark-challenge/static/main.css">
    
<link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.css" integrity="sha384-dbVIfZGuN1Yq7/1Ocstc1lUEm+AT+/rCkibIcC/OmWo5f0EA48Vf8CytHzGrSwbQ" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.js" integrity="sha384-2BKqo+exmr9su6dir+qCw08N2ZKRucY4PrGQPPWU1A7FtlCGjmEGFqXCv5nyM5Ij" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
    <script>
          document.addEventListener("DOMContentLoaded", function() {
            var blocks = document.querySelectorAll(".math.display");
            for (var i = 0; i < blocks.length; i++) {
              var b = blocks[i];
              katex.render(b.innerText, b, {displayMode:true});
            }
            blocks = document.querySelectorAll(".math.inline");
            for (var i = 0; i < blocks.length; i++) {
              var b = blocks[i];
              katex.render(b.innerText, b, {displayMode:false});
            }
          });
        </script>
  </head>
  <body>
    <h1 id="implementation-strategies">Implementation strategies</h1>
    <div class="table-of-contents">
    <ul>
    <li>
    <a href="#splitting-computation-between-the-cpu-and-gpu">1: Splitting computation between the CPU and GPU</a>
    </li>
    <li>
    <a href="#parallelism">2: Parallelism</a>
    </li>
    <li>
    <a href="#parallelism-on-the-cpu">2.1: Parallelism on the CPU</a>
    </li>
    <li>
    <a href="#parallelism-on-the-gpu">2.2: Parallelism on the GPU</a>
    </li>
    <li>
    <a href="#field-arithmetic">3: Field arithmetic</a>
    </li>
    <li>
    <a href="#optimizing-curve-arithmetic">4: Optimizing curve arithmetic</a>
    </li>
    <li>
    <a href="#representation-of-points">4.1: Representation of points</a>
    </li>
    <li>
    <a href="#exponentiation-algorithms">4.2: Exponentiation algorithms</a>
    </li>
    </ul>
    </div>
    <p>This page has suggestions for how to implement the best Groth16 SNARK prover (<a href="/snark-challenge/problem-07-groth16-prover-challenges.html">described here</a>) to take home up to $75,000 in prizes.</p>
    <h2 id="splitting-computation-between-the-cpu-and-gpu">Splitting computation between the CPU and GPU</h2>
    <p>The Groth16 prover consists of 4 <span class="math inline">G_1</span> multiexponentiations, 1 <span class="math inline">G_2</span> multiexponentiation, and 7 FFTs, as described <a href="/snark-challenge/problem-07-groth16-prover-challenges.html">here</a>.</p>
    <p>1 of the <span class="math inline">G_1</span> multiexponentiations cannot be computed until all of the FFTs. The other 3 <span class="math inline">G_1</span> multiexponentiations and the <span class="math inline">G_2</span> multiexponentiation however don't have any dependencies between each other or on any other computation.</p>
    <p>So, some of them can be computed on the CPU while at the same time others are computed on the GPU. For example, you could first the FFTs first on the CPU, while simultaneously performing 2 of the <span class="math inline">G_1</span> multi-exponentiations on the GPU. After those completed, you could then compute the final <span class="math inline">G_1</span> multi-exponentiation on the CPU and the <span class="math inline">G_2</span> multi-exponentiation on the GPU.</p>
    <h2 id="parallelism">Parallelism</h2>
    <p>Both the FFT and the multiexponentiations are massively parallelizable. The multiexponentiation in particular is an instance of a reduction: combining an array of values together using a binary operation.</p>
    <h3 id="parallelism-on-the-cpu">Parallelism on the CPU</h3>
    <p><a href="https://github.com/scipr-lab/libsnark">libsnark</a>'s &quot;sub-libraries&quot; <a href="https://github.com/scipr-lab/libff/">libff</a> and <a href="https://github.com/scipr-lab/libfqfft">libfqfft</a> implement parallelized multiexponentiation (code <a href="https://github.com/scipr-lab/libff/blob/master/libff/algebra/scalar_multiplication/multiexp.tcc#L402">here</a>) and FFT (code <a href="https://github.com/scipr-lab/libfqfft/blob/master/libfqfft/evaluation_domain/domains/basic_radix2_domain_aux.tcc#L81">here</a>) respectively.</p>
    <h3 id="parallelism-on-the-gpu">Parallelism on the GPU</h3>
    <p>Check out <a href="https://github.com/NVIDIA/cuda-samples/tree/master/Samples/reduction">this CUDA code</a> which implements a parallel reduction in CUDA to sum up an array of 32-bit ints.</p>
    <h2 id="field-arithmetic">Field arithmetic</h2>
    <p>There is an excellent CUDA implementation of modular-multiplication using Montgomery representation <a href="https://github.com/data61/cuda-fixnum">here</a>. Using that library to implement the field extension multiplication and curve-additions and then building a parallel reduction for curve-addition is likely an excellent path to creating a winning implementation of multi-exponentiation.</p>
    <h2 id="optimizing-curve-arithmetic">Optimizing curve arithmetic</h2>
    <h3 id="representation-of-points">Representation of points</h3>
    <p>There are many ways of representing curve points which yield efficiency improvements. Probably the best is <a href="">Jacobian coordinates</a> which allow for doubling points with <span class="math inline">4</span> multiplications and adding points with 12 multiplications.</p>
    <p>If some of the points are statically known, as in the case of an exponentiation, they can be represented in affine coordinates and one can take advantage of &quot;mixed-addition&quot;. Mixed-addition allows you to add a point in affine cordinates to a point in Jacobian coordinates to obtain a point in Jacobian coordinates at a cost of 8 multiplications.</p>
    <p>There are likely many other optimizations</p>
    <h3 id="exponentiation-algorithms">Exponentiation algorithms</h3>
    <p>There are many techniques for speeding up exponentiation and multi-exponentiation.</p>
  </body>
</html>